{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour commencer, assurez-vous d'avoir : \\\n",
    "    - activé l'environnement virtuel avec les commandes `cd MTB-CNN` puis `source venv/bin/activate` \\\n",
    "    - créé le kernel jupyter de l'environnement vituel en suivant les étapes suivantes : \\\n",
    "         -- executer dans le terminal `ipython kernel install --user --name=venv` \\\n",
    "         -- fermer tout les notebooks ouverts; \\\n",
    "         -- ouvrir ce notebook; \\\n",
    "         -- cliquer sur le kernel (dans le coin supérieur droit du notebook), puis choisir le kernel `venv` \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/debbiemarkslab/EVcouplings/archive/develop.zip\n",
      "  Using cached https://github.com/debbiemarkslab/EVcouplings/archive/develop.zip\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=18.2 in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (65.6.3)\n",
      "Requirement already satisfied: numpy in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (1.23.5)\n",
      "Requirement already satisfied: pandas in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (1.5.3)\n",
      "Requirement already satisfied: scipy in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (1.10.0)\n",
      "Requirement already satisfied: numba in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (0.56.4)\n",
      "Requirement already satisfied: ruamel.yaml in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (0.17.21)\n",
      "Requirement already satisfied: matplotlib in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (3.6.3)\n",
      "Requirement already satisfied: requests in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (2.28.2)\n",
      "Requirement already satisfied: mmtf-python in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (1.1.3)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (8.1.3)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (3.9.0)\n",
      "Requirement already satisfied: psutil in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (5.9.4)\n",
      "Requirement already satisfied: bokeh in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (3.0.3)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (3.1.2)\n",
      "Requirement already satisfied: biopython in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (1.81)\n",
      "Requirement already satisfied: seaborn in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (0.12.2)\n",
      "Requirement already satisfied: billiard in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (4.1.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/mamba/lib/python3.10/site-packages (from evcouplings==0.1.2) (1.2.1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /opt/mamba/lib/python3.10/site-packages (from bokeh->evcouplings==0.1.2) (9.4.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /opt/mamba/lib/python3.10/site-packages (from bokeh->evcouplings==0.1.2) (2022.9.0)\n",
      "Requirement already satisfied: contourpy>=1 in /opt/mamba/lib/python3.10/site-packages (from bokeh->evcouplings==0.1.2) (1.0.7)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /opt/mamba/lib/python3.10/site-packages (from bokeh->evcouplings==0.1.2) (6.0)\n",
      "Requirement already satisfied: tornado>=5.1 in /opt/mamba/lib/python3.10/site-packages (from bokeh->evcouplings==0.1.2) (6.1)\n",
      "Requirement already satisfied: packaging>=16.8 in /opt/mamba/lib/python3.10/site-packages (from bokeh->evcouplings==0.1.2) (22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.10/site-packages (from jinja2->evcouplings==0.1.2) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/mamba/lib/python3.10/site-packages (from pandas->evcouplings==0.1.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/mamba/lib/python3.10/site-packages (from pandas->evcouplings==0.1.2) (2022.7.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->evcouplings==0.1.2) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->evcouplings==0.1.2) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->evcouplings==0.1.2) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->evcouplings==0.1.2) (0.11.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /opt/mamba/lib/python3.10/site-packages (from mmtf-python->evcouplings==0.1.2) (1.0.4)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/mamba/lib/python3.10/site-packages (from numba->evcouplings==0.1.2) (0.39.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/mamba/lib/python3.10/site-packages (from requests->evcouplings==0.1.2) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.10/site-packages (from requests->evcouplings==0.1.2) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.10/site-packages (from requests->evcouplings==0.1.2) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.10/site-packages (from requests->evcouplings==0.1.2) (2.1.1)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /opt/mamba/lib/python3.10/site-packages (from ruamel.yaml->evcouplings==0.1.2) (0.2.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/mamba/lib/python3.10/site-packages (from scikit-learn->evcouplings==0.1.2) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/mamba/lib/python3.10/site-packages (from scikit-learn->evcouplings==0.1.2) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evcouplings==0.1.2) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/debbiemarkslab/EVcouplings/archive/develop.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/debbiemarkslab/EVcouplings/archive/develop.zip\n",
      "  Using cached https://github.com/debbiemarkslab/EVcouplings/archive/develop.zip\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -U --no-deps https://github.com/debbiemarkslab/EVcouplings/archive/develop.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous avez une erreur pour la cellule de code suivante, veuillez executer succcessivement ces deux commandes dans le terminal: \\\n",
    "    `pip install https://github.com/debbiemarkslab/EVcouplings/archive/develop.zip` \\\n",
    "    `pip install -U --no-deps https://github.com/debbiemarkslab/EVcouplings/archive/develop.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import time\n",
    "# import evcouplings\n",
    "from evcouplings.align import Alignment # install this using 'pip install evcouplings'\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_472/1013361237.py:2: DtypeWarning: Columns (3,4,7,8,9,12,17,20,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  phenotypes =  pd.read_csv(\"../input_data/master_table_resistance.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Read in list of isolates\n",
    "phenotypes =  pd.read_csv(\"../input_data/master_table_resistance.csv\")\n",
    "isolate_order = list(phenotypes.Isolate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraire les fichiers .fasta dans `MTB-CNN/input_data/` en executant la commande suivante dans le terminal `tar -xvf fasta_files.tar.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate through each fasta file, select only the positions with a high enough MAF\n",
    "# Keep only the isolates with phenotype data, reorder the alignment to match isolate_order\n",
    "# save a reduced fasta file\n",
    "!mkdir fastas_reduced\n",
    "path_to_fastas = \"../input_data/fasta_files\"\n",
    "for f in glob.glob(f\"{path_to_fastas}/*.fasta\"):\n",
    "    print(f)\n",
    "    # Reads the alignment file\n",
    "    aln = Alignment.from_file(open(f))\n",
    "    \n",
    "    # Impose a minor allele frequency threshold of 0.001 in order to keep computation tractable\n",
    "    # NOTE: the order in which you do the MAF thresholding and then subsetting the isolates matters!\n",
    "    # Here it's fine because the only isolates in my fasta files are the ones that have phenotypes\n",
    "    indices_to_keep = np.where(aln.frequencies.max(axis=1) < .999)[0]\n",
    "    subset_alignment = aln.select(indices_to_keep)\n",
    "    \n",
    "    print(\"original alignment shape\", aln.matrix.shape)\n",
    "    print(\"reduced alignment shape\", subset_alignment.matrix.shape)\n",
    "    # Cleanup giant variables\n",
    "    del aln\n",
    "\n",
    "    # Get a list of the ids we are keeping, in order\n",
    "    \n",
    "    # First, correct the ids in the alignment so that they match the table of phenotypes\n",
    "    subset_alignment.ids = [x.split(\"/\")[-1].split(\".\")[0] for x in subset_alignment.ids]\n",
    "    subset_alignment.id_to_index = {x:idx for idx,x in enumerate(subset_alignment.ids)}\n",
    "    \n",
    "    # Get the indices that would correctly reorder the alignment to match isolate_order\n",
    "    reorder_index = [\n",
    "        subset_alignment.id_to_index[x] for x in isolate_order if x in subset_alignment.id_to_index\n",
    "    ]\n",
    "    \n",
    "    # Reorder based on reorder_index\n",
    "    subset_alignment.ids = list(np.array(subset_alignment.ids)[reorder_index])\n",
    "    subset_alignment.matrix = subset_alignment.matrix[reorder_index, :]\n",
    "    print(\"shape with isolates ordered\", subset_alignment.matrix.shape)\n",
    "    \n",
    "    # Get the name of the fasta file for saving\n",
    "    name = f.split(\"/\")[-1].split(\"_\")[0]\n",
    "    \n",
    "    # save the reduced file\n",
    "    subset_alignment.write(open(f\"fastas_reduced/{name}_reduced.fa\", \"w\"))\n",
    "    np.save(f\"fastas_reduced/{name}_indices.npy\", indices_to_keep)\n",
    "    \n",
    "# Save the isolate indices in order, just in case\n",
    "pd.DataFrame(subset_alignment.ids).to_csv(\"isolate_indexes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use a major/minor encoding, keeping the number of sites the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If using one-hot, here's the functions\n",
    "\n",
    "# Mapping to use for one-hot encoding\n",
    "BASE_TO_COLUMN = {'A': 0, 'C': 1, 'T': 2, 'G': 3, '-': 4}\n",
    "\n",
    "# Get one hot vector\n",
    "def make_one_hot(matrix, BASE_TO_COLUMN=BASE_TO_COLUMN):\n",
    "    n_bases = len(BASE_TO_COLUMN.keys()) \n",
    "    one_hot = np.zeros((matrix.shape[0], matrix.shape[1]* n_bases),dtype=np.int8)\n",
    "    print(\"starting from shape\", one_hot.shape)\n",
    "    \n",
    "    for idx in range(matrix.shape[0]):\n",
    "        for jdx in range(matrix.shape[1]):\n",
    "            one_hot[idx, jdx*n_bases + BASE_TO_COLUMN[matrix[idx,jdx]]] = 1 \n",
    "\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastas_reduced/rpoBC_reduced.fa\n",
      "fastas_reduced/rpsL_reduced.fa\n",
      "fastas_reduced/gyrBA_reduced.fa\n",
      "fastas_reduced/clpC_reduced.fa\n",
      "fastas_reduced/embCAB_reduced.fa\n",
      "fastas_reduced/rrs-rrl_reduced.fa\n",
      "fastas_reduced/pncA_reduced.fa\n",
      "fastas_reduced/panD_reduced.fa\n",
      "fastas_reduced/tlyA_reduced.fa\n",
      "fastas_reduced/FabG1-inhA_reduced.fa\n",
      "fastas_reduced/KatG_reduced.fa\n",
      "fastas_reduced/gid_reduced.fa\n",
      "fastas_reduced/eis_reduced.fa\n",
      "fastas_reduced/oxyR-ahpC_reduced.fa\n",
      "fastas_reduced/ethAR_reduced.fa\n",
      "fastas_reduced/aftB-ubiA_reduced.fa\n",
      "fastas_reduced/acpM-kasA_reduced.fa\n",
      "fastas_reduced/rpsA_reduced.fa\n",
      "total sites 3011\n"
     ]
    }
   ],
   "source": [
    "# #### Make combined matrix of all the positions we'll be learning on\n",
    "\n",
    "files = ['fastas_reduced/rpoBC_reduced.fa',\n",
    " 'fastas_reduced/rpsL_reduced.fa',\n",
    " 'fastas_reduced/gyrBA_reduced.fa',\n",
    " 'fastas_reduced/clpC_reduced.fa',\n",
    " 'fastas_reduced/embCAB_reduced.fa',\n",
    " 'fastas_reduced/rrs-rrl_reduced.fa',\n",
    " 'fastas_reduced/pncA_reduced.fa',\n",
    " 'fastas_reduced/panD_reduced.fa',\n",
    " 'fastas_reduced/tlyA_reduced.fa',\n",
    " 'fastas_reduced/FabG1-inhA_reduced.fa',\n",
    " 'fastas_reduced/KatG_reduced.fa',\n",
    " 'fastas_reduced/gid_reduced.fa',\n",
    " 'fastas_reduced/eis_reduced.fa',\n",
    " 'fastas_reduced/oxyR-ahpC_reduced.fa',\n",
    " 'fastas_reduced/ethAR_reduced.fa',\n",
    " 'fastas_reduced/aftB-ubiA_reduced.fa',\n",
    " 'fastas_reduced/acpM-kasA_reduced.fa',\n",
    " 'fastas_reduced/rpsA_reduced.fa']\n",
    "\n",
    "# Compute the total number of sites in our model by summing the length of all the alignment\n",
    "total_sites = 0\n",
    "for f in files:\n",
    "    print(f)\n",
    "    aln = Alignment.from_file(open(f))\n",
    "    total_sites += aln.L\n",
    "print(\"total sites\", total_sites)\n",
    "total_seqs = aln.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23040, 3011)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Matrix to store the data for learning\n",
    "X = np.zeros((total_seqs, total_sites), dtype=np.int8)\n",
    "\n",
    "current_index = 0\n",
    "\n",
    "for f in files:\n",
    "    #print(f)\n",
    "    aln = Alignment.from_file(open(f), alphabet='-ACGT')\n",
    "    \n",
    "    # Tells you which character is the most frequent in each site\n",
    "    who_is_max = np.argmax(aln.frequencies, axis=1)\n",
    "    \n",
    "    # Major allele is encoded as 0, minor allele(s) as 1\n",
    "    major_minor = aln.matrix_mapped != who_is_max\n",
    "    \n",
    "    # Add the encoding to the X matrix\n",
    "    X[:, current_index:(current_index + major_minor.shape[1])] = major_minor\n",
    "    \n",
    "    # Keep track of how many sites in X we have filled in\n",
    "    current_index = current_index + major_minor.shape[1]\n",
    "\n",
    "np.save(\"combined_X.npy\", X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastas_reduced/rpoBC_reduced.fa\n",
      "fastas_reduced/rpsL_reduced.fa\n",
      "fastas_reduced/gyrBA_reduced.fa\n",
      "fastas_reduced/clpC_reduced.fa\n",
      "fastas_reduced/embCAB_reduced.fa\n",
      "fastas_reduced/rrs-rrl_reduced.fa\n",
      "fastas_reduced/pncA_reduced.fa\n",
      "fastas_reduced/panD_reduced.fa\n",
      "fastas_reduced/tlyA_reduced.fa\n",
      "fastas_reduced/FabG1-inhA_reduced.fa\n",
      "fastas_reduced/KatG_reduced.fa\n",
      "fastas_reduced/gid_reduced.fa\n",
      "fastas_reduced/eis_reduced.fa\n",
      "fastas_reduced/oxyR-ahpC_reduced.fa\n",
      "fastas_reduced/ethAR_reduced.fa\n",
      "fastas_reduced/aftB-ubiA_reduced.fa\n",
      "fastas_reduced/acpM-kasA_reduced.fa\n",
      "fastas_reduced/rpsA_reduced.fa\n",
      "3011 3011\n"
     ]
    }
   ],
   "source": [
    "# Make a table of all of the sites in the model for later mapping\n",
    "# Note that the sites listed here are indexed within each fasta file - NOT the MTB genome\n",
    "total_sites = []\n",
    "genes = []\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    \n",
    "    gene_name = f.split(\"fastas_reduced/\")[-1].split(\"_\")[0]\n",
    "    \n",
    "    numpy_file = f.split(\"reduced.fa\")[0] + \"indices.npy\"\n",
    "\n",
    "    sites = np.load(numpy_file)\n",
    "    \n",
    "    sites = sorted(list(sites))\n",
    "    \n",
    "    total_sites += list(sites)\n",
    "    genes += [gene_name] * len(list(sites))\n",
    "\n",
    "print(len(genes), len(total_sites))\n",
    "    \n",
    "df = pd.DataFrame({\n",
    "    \"locus\": genes,\n",
    "    \"sites\": total_sites,\n",
    "})\n",
    "\n",
    "df.to_csv(\"site_indices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23040, 3051)\n"
     ]
    }
   ],
   "source": [
    "## Create combined genotype phenotype table\n",
    "\n",
    "phenotypes =  pd.read_csv(\"/n/data2/hms/dbmi/beamlab/annachang/focus_cnn/master_table_resistance.csv\", index_col=0)\n",
    "\n",
    "# Convert phenotypes to numeric\n",
    "y_all_rs = phenotypes.fillna('-1')\n",
    "y_all_rs = y_all_rs.astype(str)\n",
    "resistance_categories = {'R': 0, 'S': 1, '-1.0': -1, '-1': -1}\n",
    "\n",
    "y_all = y_all_rs.copy()\n",
    "for key, val in resistance_categories.items():\n",
    "    y_all[y_all_rs == key] = val\n",
    "\n",
    "# Fill missing data with nans\n",
    "y_all[y_all==-1] = np.nan\n",
    "phenotypes=y_all\n",
    "\n",
    "# read in X matrix and isolate indexes \n",
    "X_all = np.load(\"combined_X.npy\")\n",
    "\n",
    "# Create a dataframe of X's with one row per isolate\n",
    "indices = pd.read_csv(\"isolate_indexes.csv\", index_col=0)\n",
    "X_df = pd.concat([indices, pd.DataFrame(X_all)], axis=1)\n",
    "\n",
    "# Add column names corresponding to position\n",
    "column_names = pd.read_csv(\"site_indices.csv\", index_col=0)\n",
    "colnames = [\"isolate\"] + [f\"{x}_{y}\" for x,y in zip(column_names.locus, column_names.sites)]\n",
    "X_df.columns = colnames\n",
    "\n",
    "# Merge with phenotypes and save\n",
    "X_df = X_df.merge(phenotypes, right_on=\"Isolate\", left_on=\"isolate\", how=\"left\")\n",
    "print(X_df.shape)\n",
    "X_df.to_csv(\"combined_geno_pheno_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
